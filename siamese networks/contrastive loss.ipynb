{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:21.922583Z",
     "start_time": "2021-05-03T21:45:18.871215Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from random import choice, shuffle, sample\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "!pip install livelossplot --quiet\n",
    "from livelossplot import PlotLosses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T20:57:53.938945Z",
     "start_time": "2021-05-03T20:57:53.926976Z"
    }
   },
   "outputs": [],
   "source": [
    "def images_paths(path):\n",
    "    '''Retrieves the paths of all available images from path folder'''\n",
    "    imgs_paths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if(file.endswith(\".jpg\")):\n",
    "                imgs_paths.append(os.path.join(root,file))\n",
    "    return imgs_paths\n",
    "\n",
    "def path_to_label(path, normal = False):\n",
    "    '''Encodes each category label from the path of each image'''\n",
    "    if normal:\n",
    "        path = path.split('/')\n",
    "    else:\n",
    "        path = path.split('\\\\')\n",
    "    label = path[2]\n",
    "    for w in path[3:-1]:\n",
    "        if not w.endswith('.jpg'):\n",
    "            label = label + '-' + w        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, support and query datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T20:59:41.477481Z",
     "start_time": "2021-05-03T20:59:41.376267Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = ''\n",
    "\n",
    "# All brands training, evaluation on Givenchy\n",
    "df_all_brands = pd.read_csv('all_brands_but_givenchy.csv', index_col=False, usecols=['path', 'label'])\n",
    "support_givenchy = pd.read_csv('support_givenchy.csv', index_col=False, usecols=['path', 'label'])\n",
    "query_givenchy = pd.read_csv('query_givenchy.csv', index_col=False, usecols=['path', 'label'])\n",
    "\n",
    "# Evaluation on Versace\n",
    "def local_path(x):\n",
    "    m = len('/content/drive/MyDrive/')\n",
    "    new_path = 'Navee Dataset/drive_data/' + x[m:]\n",
    "    return new_path\n",
    "\n",
    "# Versace train\n",
    "train_df = pd.read_csv(ROOT+'train_100_categories.csv', index_col=False, usecols=['path', 'label'])\n",
    "train_df.path = train_df.path.apply(lambda x:local_path(x))\n",
    "train_df.label = train_df.path.apply(lambda x:path_to_label(x))\n",
    "\n",
    "query_versace = pd.read_csv(ROOT+'query_50_categories.csv', index_col=False, usecols=['path', 'label'])\n",
    "support_versace = pd.read_csv(ROOT+'support_50_categories.csv', index_col=False, usecols=['path', 'label'])\n",
    "query_versace.path = query_versace.path.apply(lambda x:local_path(x))\n",
    "support_versace.path = support_versace.path.apply(lambda x:local_path(x))\n",
    "query_versace.label = query_versace.path.apply(lambda x:path_to_label(x,True))\n",
    "support_versace.label = support_versace.path.apply(lambda x:path_to_label(x,True))\n",
    "\n",
    "#DO NOT TRAIN ON VERSACE QUERY/SUPPORT IMAGES\n",
    "df_all_brands = df_all_brands.loc[~df_all_brands.label.isin(support_versace.label.values)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:27:18.131249Z",
     "start_time": "2021-05-03T21:27:18.113298Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiameseResnet(nn.Module):\n",
    "    ''' Siamese network to learn images representation'''\n",
    "    def __init__(self):\n",
    "        super(SiameseResnet, self).__init__()\n",
    "        \n",
    "        # Loading ResNet\n",
    "        model = models.resnet18(pretrained=True)\n",
    "            \n",
    "        # Removing last fully-connected layer\n",
    "        model.fc = nn.Sequential()\n",
    "        self.extractor = model\n",
    "        \n",
    "        # Fully-connected layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(512,512),nn.LeakyReLU())\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output_1 = self.forward_one(x1)\n",
    "        output_2 = self.forward_one(x2)\n",
    "        return output_1, output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:05.742248Z",
     "start_time": "2021-05-03T21:45:05.717800Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    ''' Implements dataset creation for siamese network.'''\n",
    "\n",
    "    def __init__(self,df_paths_labels,length,chosen_labels=None,transform=None, p=0.5):\n",
    "        self.df = df_paths_labels\n",
    "        self.len_df = len(df_paths_labels)    \n",
    "        if chosen_labels is not None:\n",
    "            self.chosen_labels = chosen_labels\n",
    "        else:\n",
    "            self.chosen_labels = df_paths_labels.label.unique()\n",
    "        self.length = length\n",
    "        self.transform = transform\n",
    "        self.fraction_same = p # Proportion of positive pairs fed during training\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        '''Selects first label at random. Second image depends on positive pairs proportion wanted'''\n",
    "        path_1 = random.choice(self.df.loc[self.df.label.isin(self.chosen_labels)].path.values)\n",
    "        label_1 = path_to_label(path_1)\n",
    "        # Dataset with a fraction p of positively labeled pairs\n",
    "        same_label = random.random()\n",
    "        same_label = int(same_label < self.fraction_same)\n",
    "\n",
    "        if same_label:\n",
    "            # Picks image from the same label as the first one\n",
    "            path_2 = random.choice(self.df.loc[(self.df.label == label_1) & ~(self.df.path == path_1)].path.values)\n",
    "            y = torch.from_numpy(np.array([0],dtype=np.float32))\n",
    "        else:\n",
    "            # Picks image from a different label\n",
    "            path_2 = random.choice(self.df.loc[(self.df.label != label_1)].path.values)\n",
    "            y = torch.from_numpy(np.array([1],dtype=np.float32))\n",
    "\n",
    "        img_1 = Image.open(path_1).convert(\"RGB\")\n",
    "        img_2 = Image.open(path_2).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img_1 = self.transform(img_1)\n",
    "            img_2 = self.transform(img_2)\n",
    "        \n",
    "        return img_1, img_2 , y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:06.923860Z",
     "start_time": "2021-05-03T21:45:06.913496Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    ''' Implements contrastive loss to train Siamese Network'''\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output_1, output_2, label):\n",
    "        # Distance between embedded outputs\n",
    "        L2_distance = F.pairwise_distance(output_1, output_2, keepdim = True)\n",
    "        \n",
    "        # Loss calculation\n",
    "        losses = (1-label) * torch.pow(L2_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - L2_distance, min=0.0), 2)\n",
    "        contrastive_loss = torch.mean(losses)\n",
    "\n",
    "        return contrastive_loss\n",
    "\n",
    "def contrastive_batch_loss(output_1, output_2, label, margin=2.0 ):\n",
    "    ''' Computes the loss for each pair of images in the batch, not returning mean but a tensor with losses to sort them afterwards'''\n",
    "    L2_distance = F.pairwise_distance(output_1, output_2, keepdim = True)\n",
    "    contrastive_loss = (1-label) * torch.pow(L2_distance, 2) + (label) * torch.pow(torch.clamp(margin - L2_distance, min=0.0), 2)\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:07.499363Z",
     "start_time": "2021-05-03T21:45:07.480384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting of seeds\n",
    "def enforce_all_seeds(seed):\n",
    "    '''Forces seeds for reproducibility and consistancy in our results'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    rgen = np.random.default_rng(seed)\n",
    "    return rgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:07.964913Z",
     "start_time": "2021-05-03T21:45:07.945027Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_average_precision(net, support_df, query_df, custom_transforms):\n",
    "    '''Computes mAP for the given network on support and query datasets'''\n",
    "    \n",
    "    transformer = custom_transforms['val']\n",
    "\n",
    "    def preprocess(path):\n",
    "        '''Returns image in tensor formed ready to be fed to Siamese network'''\n",
    "        return transformer(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "    def forward_pass(path, net):\n",
    "        '''Performs a fordward pass into the net'''\n",
    "        img = preprocess(path)\n",
    "        y = net.forward_one(img.unsqueeze(0).cuda())\n",
    "        y = y.detach().cpu().numpy()\n",
    "        return y\n",
    "\n",
    "    support_df['embedded_images'] = support_df.path.apply(lambda x:forward_pass(x, net))\n",
    "    query_df['embedded_images'] = query_df.path.apply(lambda x:forward_pass(x, net))\n",
    "\n",
    "    def calculate_AP(label):\n",
    "        '''calculates AP for the given label'''\n",
    "        # Ground truth vector\n",
    "        y_ground = support_df.label.apply(lambda x: 1 if x==label else 0).values\n",
    "        \n",
    "        # Embedded query\n",
    "        img_embedded = query_df.embedded_images.loc[query_df.label == label].values[0]\n",
    "                \n",
    "        def distance_to_query(x):\n",
    "            return -np.linalg.norm(x-img_embedded)\n",
    "        \n",
    "        # Distances vector\n",
    "        y_distances = support_df.embedded_images.apply(lambda x: distance_to_query(x))\n",
    "        return average_precision_score(y_ground, y_distances)\n",
    "\n",
    "    query_df['AP'] = query_df.label.apply(lambda x:calculate_AP(x))\n",
    "    mAP = np.mean(query_df['AP'].values)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T21:45:25.326495Z",
     "start_time": "2021-05-03T21:45:24.642448Z"
    }
   },
   "outputs": [],
   "source": [
    "rgen = enforce_all_seeds(42)\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "data = SiameseDataset(train_df, \n",
    "                      length = 10000,\n",
    "                      chosen_labels = None,     \n",
    "                      transform = data_transforms['train'], \n",
    "                      p=0.5)\n",
    "\n",
    "siamese_dataloader = DataLoader(data,\n",
    "                                shuffle=False,\n",
    "                                num_workers=0,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "net = SiameseResnet().cuda()\n",
    "\n",
    "# Training logs\n",
    "loss_history = []\n",
    "hard_examples_history = []\n",
    "mean_loss_history = []\n",
    "nb_batchs = data.length//batch_size\n",
    "plt.rcParams['figure.figsize'] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Keeping logs\n",
    "today = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "exp_name = 'resnet_full_retrain_'\n",
    "weights_file =  exp_name + str(today) +'.pt'\n",
    "CSV_logs_file = exp_name + str(today) +'.csv'\n",
    "df_logs = pd.DataFrame({'epoch':[], 'batch':[], 'loss':[], 'hard_loss':[], 'versace_mAP':[], 'givenchy_mAP':[] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "top_k = int(batch_size*0.4) # Proportion of elements in each batch used to train the network\n",
    "margin = 4.0 # Loss margin\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.00005) # Original = 0.00005\n",
    "groups = {'contrastive loss': ['mean loss', 'hard_examples loss', 'last batch loss'], 'mAP': ['Versace mAP', 'Givenchy mAP']}\n",
    "liveloss = PlotLosses(groups=groups)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    mean_epoch_loss = []\n",
    "    for i, batch in enumerate(siamese_dataloader):\n",
    "        net.train()\n",
    "        img_1, img_2 , label = batch\n",
    "        img_1, img_2 , label = img_1.cuda(), img_2.cuda() , label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Computes the loss for each batch element, sorts the batch to get negatives examples\n",
    "            temp_output_1, temp_output_2 = net(img_1,img_2)\n",
    "            temp_loss_contrastive = contrastive_batch_loss(temp_output_1,temp_output_2,label)\n",
    "            mean_epoch_loss.append(torch.mean(temp_loss_contrastive).item())      \n",
    "            loss, indexes = torch.sort(temp_loss_contrastive, dim=0)\n",
    "\n",
    "        # Negative mining: retrieves indexes of the given % of worse examples\n",
    "        indexes = indexes[-top_k:] \n",
    "        input_1, input_2 = img_1[indexes].squeeze(), img_2[indexes].squeeze()\n",
    "        mined_labels = label[indexes].squeeze(dim=2)\n",
    "\n",
    "        # Computes forward pass on selected pairs.\n",
    "        output_1, output_2 = net(input_1,input_2)\n",
    "        loss_contrastive = criterion(output_1,output_2,mined_labels)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"\\r Epoch number {epoch+1}/{EPOCHS}, batch number {i+1}/{int(data.length/batch_size)}, current hard loss={loss_contrastive.item(): .5}, batch loss={mean_epoch_loss[-1]: .5}\", end='')\n",
    "\n",
    "        # Logs update: CSV files and weights are saved at each epoch\n",
    "        net.eval()\n",
    "        hard_examples_history.append(loss_contrastive)\n",
    "        loss_history.append(torch.mean(temp_loss_contrastive))\n",
    "        if i + 1 != nb_batchs:\n",
    "            df_logs = df_logs.append(pd.DataFrame({'epoch':[epoch+1], 'batch':[i+1], 'loss':[mean_epoch_loss[-1]], 'hard_loss':[loss_contrastive.item()], 'versace_mAP':[np.nan], 'givenchy_mAP':[np.nan]}))\n",
    "        else:\n",
    "            versace_mAP = mean_average_precision(net, support_versace, query_versace, data_transforms)\n",
    "            givenchy_mAP = mean_average_precision(net, support_givenchy, query_givenchy, data_transforms)\n",
    "            df_logs = df_logs.append(pd.DataFrame({'epoch':[epoch+1], 'batch':[i+1], 'loss':[mean_epoch_loss[-1]], 'hard_loss':[loss_contrastive.item()], 'versace_mAP':[versace_mAP], 'givenchy_mAP':[givenchy_mAP]}))\n",
    "\n",
    "    mean_loss_history.append(np.mean(mean_epoch_loss))\n",
    "    df_logs.to_csv(ROOT + 'training_logs_Navee/' + CSV_logs_file, index=False)\n",
    "    \n",
    "    # Livelossplot logs\n",
    "    liveloss.update({'mean loss': np.mean(mean_epoch_loss),\n",
    "                     'hard_examples loss': loss_contrastive.item(),\n",
    "                     'last batch loss': mean_epoch_loss[-1],\n",
    "                     'Versace mAP': versace_mAP,\n",
    "                     'Givenchy mAP': givenchy_mAP})\n",
    "    liveloss.send()\n",
    "    torch.save(net,ROOT+'training_logs_Navee/'+weights_file)\n",
    "end = time.time()\n",
    "print(f'\\n Training time {end-start: .5}s, that is {int((end-start)//3600)}h{int((end-start)%3600/60)}min')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
